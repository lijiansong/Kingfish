nohup: ignoring input
2019-01-22 08:49:25.962069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-22 08:49:25.962438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-22 08:49:25.962457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-22 08:49:26.323948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-22 08:49:26.324009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-22 08:49:26.324018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-22 08:49:26.324345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Namespace(batch_size=128, beam_width=10, corpus_name='GooglePlay', data_path='data', dropout=0.25, embedding_size=128, eval_file='data/dev.txt', learning_rate=0.001, max_dialog_len=5, max_utterance_len=35, num_epochs=15, num_hidden_layers=1, num_hidden_units=256, restore=False, restore_path='model/model.ckpt', root_path='model_v0', save_path='model/model.ckpt', test_file='data/test.txt', train_file='data/train.txt', use_beam_search=False)
Building the TensorFlow graph...
Building the TensorFlow graph...
TensorFlow variables initialized.
[2019-01-22 08:49:35.546483] batch=0100, loss=5.5593
[2019-01-22 08:49:40.243205] batch=0200, loss=4.4948
[2019-01-22 08:49:44.987967] batch=0300, loss=3.5239
[2019-01-22 08:49:49.741330] batch=0400, loss=3.0556
[2019-01-22 08:49:54.487948] batch=0500, loss=2.4956
[2019-01-22 08:49:59.294768] batch=0600, loss=2.1817
[2019-01-22 08:50:04.095781] batch=0700, loss=1.5052
Epoch=1, train_loss=3.5476
Epoch=1, eval_loss=5.4605
Saving the trained model...
Minimum loss reduced, save model, min_loss=5.4605
[2019-01-22 08:50:09.624946] batch=0100, loss=1.3529
[2019-01-22 08:50:14.388078] batch=0200, loss=1.1438
[2019-01-22 08:50:19.092279] batch=0300, loss=0.9788
[2019-01-22 08:50:23.868368] batch=0400, loss=0.9949
[2019-01-22 08:50:28.525782] batch=0500, loss=0.8906
[2019-01-22 08:50:33.234336] batch=0600, loss=0.8924
[2019-01-22 08:50:38.033154] batch=0700, loss=0.6475
Epoch=2, train_loss=1.0554
Epoch=2, eval_loss=5.7497
[2019-01-22 08:50:43.158761] batch=0100, loss=0.6466
[2019-01-22 08:50:47.893293] batch=0200, loss=0.5589
[2019-01-22 08:50:52.584534] batch=0300, loss=0.4959
[2019-01-22 08:50:57.308569] batch=0400, loss=0.5547
[2019-01-22 08:51:02.068344] batch=0500, loss=0.4963
[2019-01-22 08:51:06.852914] batch=0600, loss=0.5111
[2019-01-22 08:51:11.650295] batch=0700, loss=0.3979
Epoch=3, train_loss=0.5665
Epoch=3, eval_loss=5.9702
[2019-01-22 08:51:16.831022] batch=0100, loss=0.4387
[2019-01-22 08:51:21.485211] batch=0200, loss=0.3959
[2019-01-22 08:51:26.098370] batch=0300, loss=0.3488
[2019-01-22 08:51:30.751718] batch=0400, loss=0.4043
[2019-01-22 08:51:35.378467] batch=0500, loss=0.3613
[2019-01-22 08:51:40.027924] batch=0600, loss=0.3813
[2019-01-22 08:51:44.839016] batch=0700, loss=0.2823
Epoch=4, train_loss=0.3953
Epoch=4, eval_loss=6.1481
[2019-01-22 08:51:49.919234] batch=0100, loss=0.3529
[2019-01-22 08:51:54.930953] batch=0200, loss=0.3298
[2019-01-22 08:51:59.872246] batch=0300, loss=0.2990
[2019-01-22 08:52:04.555909] batch=0400, loss=0.3169
[2019-01-22 08:52:09.252967] batch=0500, loss=0.2909
[2019-01-22 08:52:13.989563] batch=0600, loss=0.3163
[2019-01-22 08:52:18.644573] batch=0700, loss=0.2416
Epoch=5, train_loss=0.3134
Epoch=5, eval_loss=6.2920
[2019-01-22 08:52:23.696124] batch=0100, loss=0.2906
[2019-01-22 08:52:28.277464] batch=0200, loss=0.3039
[2019-01-22 08:52:32.887040] batch=0300, loss=0.2522
[2019-01-22 08:52:37.493995] batch=0400, loss=0.2691
[2019-01-22 08:52:42.146680] batch=0500, loss=0.2405
[2019-01-22 08:52:46.739946] batch=0600, loss=0.2837
[2019-01-22 08:52:51.391532] batch=0700, loss=0.2143
Epoch=6, train_loss=0.2653
Epoch=6, eval_loss=6.3843
[2019-01-22 08:52:56.517126] batch=0100, loss=0.2681
[2019-01-22 08:53:01.152004] batch=0200, loss=0.2704
[2019-01-22 08:53:05.803310] batch=0300, loss=0.2423
[2019-01-22 08:53:10.505125] batch=0400, loss=0.2413
[2019-01-22 08:53:15.160796] batch=0500, loss=0.2229
[2019-01-22 08:53:19.844674] batch=0600, loss=0.2435
[2019-01-22 08:53:24.612928] batch=0700, loss=0.1973
Epoch=7, train_loss=0.2365
Epoch=7, eval_loss=6.4343
[2019-01-22 08:53:29.656945] batch=0100, loss=0.2349
[2019-01-22 08:53:34.399001] batch=0200, loss=0.2516
[2019-01-22 08:53:39.015829] batch=0300, loss=0.1984
[2019-01-22 08:53:43.677775] batch=0400, loss=0.2183
[2019-01-22 08:53:48.402323] batch=0500, loss=0.2017
[2019-01-22 08:53:53.088600] batch=0600, loss=0.2245
[2019-01-22 08:53:57.822136] batch=0700, loss=0.1808
Epoch=8, train_loss=0.2167
Epoch=8, eval_loss=6.5935
[2019-01-22 08:54:02.946280] batch=0100, loss=0.2156
[2019-01-22 08:54:07.640184] batch=0200, loss=0.2193
[2019-01-22 08:54:12.274240] batch=0300, loss=0.1894
[2019-01-22 08:54:16.998374] batch=0400, loss=0.2044
[2019-01-22 08:54:21.726129] batch=0500, loss=0.1878
[2019-01-22 08:54:26.472931] batch=0600, loss=0.2131
[2019-01-22 08:54:31.169049] batch=0700, loss=0.1676
Epoch=9, train_loss=0.2026
Epoch=9, eval_loss=6.7131
[2019-01-22 08:54:36.176341] batch=0100, loss=0.2028
[2019-01-22 08:54:40.907924] batch=0200, loss=0.2220
[2019-01-22 08:54:45.595371] batch=0300, loss=0.1962
[2019-01-22 08:54:50.250799] batch=0400, loss=0.1999
[2019-01-22 08:54:55.045375] batch=0500, loss=0.1842
[2019-01-22 08:54:59.721756] batch=0600, loss=0.2046
[2019-01-22 08:55:04.632124] batch=0700, loss=0.1581
Epoch=10, train_loss=0.1922
Epoch=10, eval_loss=6.8255
[2019-01-22 08:55:09.671943] batch=0100, loss=0.1881
[2019-01-22 08:55:14.345549] batch=0200, loss=0.2079
[2019-01-22 08:55:18.927134] batch=0300, loss=0.1876
[2019-01-22 08:55:23.636666] batch=0400, loss=0.2027
[2019-01-22 08:55:28.288728] batch=0500, loss=0.1765
[2019-01-22 08:55:32.905333] batch=0600, loss=0.1952
[2019-01-22 08:55:37.548989] batch=0700, loss=0.1513
Epoch=11, train_loss=0.1838
Epoch=11, eval_loss=6.9550
[2019-01-22 08:55:42.465640] batch=0100, loss=0.1791
[2019-01-22 08:55:47.032312] batch=0200, loss=0.2102
[2019-01-22 08:55:51.726975] batch=0300, loss=0.1703
[2019-01-22 08:55:56.384979] batch=0400, loss=0.1856
[2019-01-22 08:56:01.121005] batch=0500, loss=0.1677
[2019-01-22 08:56:05.909322] batch=0600, loss=0.1835
[2019-01-22 08:56:10.765361] batch=0700, loss=0.1530
Epoch=12, train_loss=0.1770
Epoch=12, eval_loss=7.0321
[2019-01-22 08:56:15.847039] batch=0100, loss=0.1720
[2019-01-22 08:56:20.489380] batch=0200, loss=0.1986
[2019-01-22 08:56:25.096035] batch=0300, loss=0.1649
[2019-01-22 08:56:29.808696] batch=0400, loss=0.1908
[2019-01-22 08:56:34.539374] batch=0500, loss=0.1604
[2019-01-22 08:56:39.151745] batch=0600, loss=0.1801
[2019-01-22 08:56:43.857936] batch=0700, loss=0.1543
Epoch=13, train_loss=0.1721
Epoch=13, eval_loss=7.0792
[2019-01-22 08:56:48.942234] batch=0100, loss=0.1684
[2019-01-22 08:56:53.650261] batch=0200, loss=0.1873
[2019-01-22 08:56:58.333368] batch=0300, loss=0.1617
[2019-01-22 08:57:03.245184] batch=0400, loss=0.1802
[2019-01-22 08:57:08.194714] batch=0500, loss=0.1588
[2019-01-22 08:57:13.020206] batch=0600, loss=0.1784
[2019-01-22 08:57:17.779345] batch=0700, loss=0.1497
Epoch=14, train_loss=0.1679
Epoch=14, eval_loss=7.1045
[2019-01-22 08:57:22.875605] batch=0100, loss=0.1617
[2019-01-22 08:57:27.592430] batch=0200, loss=0.2003
[2019-01-22 08:57:32.152378] batch=0300, loss=0.1659
[2019-01-22 08:57:36.685912] batch=0400, loss=0.1805
[2019-01-22 08:57:41.235820] batch=0500, loss=0.1603
[2019-01-22 08:57:45.798249] batch=0600, loss=0.1706
[2019-01-22 08:57:50.468114] batch=0700, loss=0.1476
Epoch=15, train_loss=0.1639
Epoch=15, eval_loss=7.1820
Time to train model is 8.4324 minutes.
>>> 
